{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4oP1z9rThbl"
   },
   "source": [
    "Source to following code: https://pythonprogramming.net/convolutional-neural-network-deep-learning-python-tensorflow-keras/\n",
    "<br>\n",
    "Second source is: https://towardsdatascience.com/deeppicar-part-5-lane-following-via-deep-learning-d93acdce6110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqbtLkl9aV7l"
   },
   "outputs": [],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQL2HHfUaoTa"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbONPD1gaqeB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import statistics\n",
    "import gc # Helps to clear up some ram\n",
    "\n",
    "import cv2\n",
    "from numpy import random\n",
    "from imgaug import augmenters as img_aug\n",
    "\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qa9Be4Gza57a"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8CXsMZ8bLhy"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = \"/content/drive/My Drive/AutonomousDriving\"\n",
    "pickle_in = open(path+\"/trainColour.pickle\",\"rb\")\n",
    "# Remove binary version\n",
    "trainImagesAndLabels = pickle.load(pickle_in)\n",
    "\n",
    "imageId = []\n",
    "X = []\n",
    "y_angle = []\n",
    "y_speed = []\n",
    "for id,image,lbl_angle,lbl_speed in trainImagesAndLabels:\n",
    "  #checking = image\n",
    "  if np.count_nonzero(pd.isnull(image)): \n",
    "    print(\"NA: This image was not read well so we will skip for now\")\n",
    "  else:\n",
    "    imageId.append(id)\n",
    "    X.append(image)\n",
    "    y_angle.append(lbl_angle)\n",
    "    y_speed.append(lbl_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DN4f-rGJML0l"
   },
   "source": [
    "Standard deviation of the angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI1Gn6MeNBIe"
   },
   "outputs": [],
   "source": [
    "plt.hist(y_speed)\n",
    "statistics.stdev(y_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRL3lMMfbwZa"
   },
   "outputs": [],
   "source": [
    "def pan(image):\n",
    "    # pan left / right / up / down about 10%\n",
    "    pan = img_aug.Affine(translate_percent= {\"x\" : (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n",
    "    image = pan.augment_image(image)\n",
    "    return image\n",
    "    \n",
    "def zoom(image):\n",
    "    zoom = img_aug.Affine(scale=(1, 1.3))  # zoom from 100% (no zoom) to 130%\n",
    "    image = zoom.augment_image(image)\n",
    "    return image\n",
    "\n",
    "def adjust_brightness(image):\n",
    "    # increase or decrease brightness by 30%\n",
    "    brightness = img_aug.Multiply((0.7, 1.3))\n",
    "    image = brightness.augment_image(image)\n",
    "    return image\n",
    "\n",
    "def blur(image):\n",
    "    kernel_size = random.randint(1, 5)  # kernel larger than 5 would make the image way too blurry\n",
    "    image = cv2.blur(image,(kernel_size, kernel_size))\n",
    "   \n",
    "    return image\n",
    "\n",
    "\n",
    "def random_flip(image,speed):\n",
    "    is_flip = random.randint(0, 1)\n",
    "    if is_flip == 1:\n",
    "        # randomly flip horizon\n",
    "        image = cv2.flip(image,1)\n",
    "        \n",
    "   \n",
    "    return image, speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k9m29RzXlVg"
   },
   "outputs": [],
   "source": [
    "def random_augment(image, speed):\n",
    "  \n",
    "    if np.random.rand() < 0.5:\n",
    "        image = pan(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = zoom(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = blur(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = adjust_brightness(image)\n",
    "    image, speed = random_flip(image, speed)\n",
    "    \n",
    "    return image, speed\n",
    "# imageId = np.array(imageId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8NzgqqfXmdj"
   },
   "outputs": [],
   "source": [
    "smallSpeeds=(np.where(np.array(y_speed)<0.5)[0])\n",
    "# # bigSpeeds=(np.where(np.array(y_speed)>0.5)[0])\n",
    "\n",
    "\n",
    "augmentedImages = []\n",
    "augmentedSpeed = []\n",
    "augmentedID = []\n",
    "# for idx in bigSpeeds:\n",
    "#     for _ in range(1):\n",
    "#       image, speed = random_augment(X[idx], y_speed[idx])\n",
    "#       augmentedImages.append(image)\n",
    "#       augmentedSpeed.append(speed)\n",
    "#       augmentedID.append(imageId[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_W1wyaojEjC"
   },
   "outputs": [],
   "source": [
    "for idx in smallSpeeds:\n",
    "    image, speed = random_augment(X[idx], y_speed[idx])\n",
    "    augmentedImages.append(image)\n",
    "    augmentedSpeed.append(speed)\n",
    "    augmentedID.append(imageId[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNG3bBHRdwWd"
   },
   "source": [
    "Hist before augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WIHzvBWdvIj"
   },
   "outputs": [],
   "source": [
    "plt.hist(y_speed)\n",
    "statistics.stdev(y_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhRi4RcWd18_"
   },
   "source": [
    "Hist after augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMUzEaSdd1SD"
   },
   "outputs": [],
   "source": [
    "for idx in range(len(augmentedSpeed)):\n",
    "  y_speed.append(augmentedSpeed[idx])\n",
    "  X.append(augmentedImages[idx])\n",
    "  imageId.append(augmentedID[idx])\n",
    "del augmentedImages\n",
    "del augmentedSpeed\n",
    "del augmentedID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofJ9GNwQfWUV"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.array(y_speed))\n",
    "statistics.stdev(y_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UopdowkSSTNi"
   },
   "outputs": [],
   "source": [
    "X_new = X\n",
    "y_speed_new = y_speed\n",
    "idImage = imageId\n",
    "del X\n",
    "del y_speed\n",
    "del imageId\n",
    "del pickle_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CesI0uwdNNAh"
   },
   "outputs": [],
   "source": [
    "print(idImage[-1])\n",
    "print(y_speed_new[-1])\n",
    "plt.imshow(X_new[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZN9lKo_lXYq"
   },
   "outputs": [],
   "source": [
    "# print(len(X_new))\n",
    "# print(len(y_angle_new))\n",
    "# print(len(idImage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11h4n49QZ8Aw"
   },
   "outputs": [],
   "source": [
    "X_new = np.array(X_new)\n",
    "y_speed_new = np.array(y_speed_new)\n",
    "idImage = np.array(idImage)\n",
    "# print(X_new.shape)\n",
    "# print(y_angle_new.shape)\n",
    "# print(idImage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irHogzFCABqi"
   },
   "outputs": [],
   "source": [
    "print(idImage[-1])\n",
    "print(y_speed_new[-1])\n",
    "plt.imshow(X_new[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itULCrLif3_4"
   },
   "source": [
    "Shuffle the two X_new and y_angle arrays. Note that sklearn shuffle allows us to shuffle two arrays as they were together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsr6OUWipy7l"
   },
   "source": [
    "Preprocces images to scale, normalize change RGB to YUV and add gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-Fy5sMpE01m"
   },
   "outputs": [],
   "source": [
    "def img_preprocess(image):\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # in the case that there exists an object in the image of interest DO NOT MAKE reduce the height of the image!!\n",
    "    \n",
    "    # image = image[int(height/2):,:,:]  # remove top half of the image, as it is not relevant for lane following\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)  # Nvidia model said it is best to use YUV color space\n",
    "    image = cv2.GaussianBlur(image, (3,3), 0) # Gaussian Noise / filtering\n",
    "    image = cv2.resize(image, (240,114))# Need to reduce  size to conserve memory\n",
    "    image = cv2.resize(image, (0,0), fx=0.7, fy=0.7)# Need to reduce  size to conserve memory\n",
    "    image = image / 255 # normalizing\n",
    "    # Round everything into the image to 1 decimal place\n",
    "    image = np.around(image ,2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhzE5GJiEQck"
   },
   "outputs": [],
   "source": [
    "# img = cv2.resize(X_new[0], (240,113))\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II_uY56MpqJK"
   },
   "outputs": [],
   "source": [
    "# for idx in range(len(X_new)):\n",
    "#   if X_new[idx].shape == (240,320,4):\n",
    "#     X_new[idx] = X_new[idx][:,:,0:3]\n",
    "X_final = []\n",
    "for img_index in range(X_new.shape[0] ):\n",
    "  X_f = img_preprocess(X_new[img_index,:,:,:])\n",
    "  X_final.append(X_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB1BaRBjrUPn"
   },
   "outputs": [],
   "source": [
    "X_final = np.array(X_final)\n",
    "# X_final = np.array(X_final, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AngAeTNDHAzJ"
   },
   "source": [
    "Create a test dataset to ensure that we do not overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoCUio-vBn-R"
   },
   "outputs": [],
   "source": [
    "print(idImage[0])\n",
    "print(y_speed_new[0])\n",
    "plt.imshow(X_final[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cell below for training\n",
    "### Run two cells below for creating final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tCXVgZHGrMU"
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split( X_final, y_speed_new, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQSArvBRVDG3"
   },
   "outputs": [],
   "source": [
    "#shuffle x and y together\n",
    "perms = np.random.permutation(len(X_final))\n",
    "X_train = X_final[perms]\n",
    "y_train = y_speed_new[perms]\n",
    "del y_speed_new\n",
    "del X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzmwoMX69ZJh"
   },
   "outputs": [],
   "source": [
    "idImage = idImage[perms]\n",
    "plt.imshow(X_train[22])#,:,:,:])\n",
    "print(y_train[22])\n",
    "print(idImage[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3QsocHmqtdI"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "tf_y_train = tf.reshape(y_train,[ y_train.shape[0] ,1 ]).numpy()\n",
    "print(y_train.shape)\n",
    "print(tf_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTCGDtJKFV3N"
   },
   "outputs": [],
   "source": [
    "def nvidia_model():\n",
    "    model = Sequential(name='Nvidia_Model')\n",
    "    \n",
    "    # elu=Exponential Linear Unit, similar to leaky Relu\n",
    "    # skipping 1st hiddel layer (nomralization layer), as we have normalized the data\n",
    "    \n",
    "    # Convolution Layers\n",
    "    model.add(Conv2D(24, (5, 5), strides=(2, 2), input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]), activation='elu')) \n",
    "    #model.add(Conv2D(24, (5, 5), strides=(2, 2), input_shape=(120, 320, 3), activation='elu')) \n",
    "    model.add(Conv2D(36, (5, 5), strides=(2, 2), activation='elu')) \n",
    "    model.add(Conv2D(48, (5, 5), strides=(2, 2), activation='elu')) \n",
    "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "    #model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dropout(0.1))\n",
    "    #model.add(Dense(200, activation='elu')) # NOT PART OF NVIDIA\n",
    "    #model.add(Dense(200, activation='elu'))\n",
    "\n",
    "    model.add(Dropout(0.1)) # not in original model. added for more robustness\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    #model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    #model.add(Dense(50, activation='elu'))\n",
    "    #model.add(Dropout(0.05))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    #model.add(Dense(10, activation='elu'))\n",
    "    \n",
    "    # output layer: turn angle (from 45-135, 90 is straight, <90 turn left, >90 turn right)\n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    \n",
    "    #optimizer = Adam(lr=1e-7) # lr is learning rate\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,               \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = nvidia_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5uNaoOaGDcr"
   },
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "        patience=7, verbose=1, mode='auto',\n",
    "        restore_best_weights=True)\n",
    "model.fit(X_train, tf_y_train, batch_size=128, epochs=100, validation_split=0.2,callbacks=[monitor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLC3dAExK2Lm"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x=X_test,y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFugzsUCWD6x"
   },
   "outputs": [],
   "source": [
    "speed = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIiUi8J3xyaP"
   },
   "outputs": [],
   "source": [
    "speed = np.round(speed,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKF5Qh9fnhuQ"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6iHnE1yXBh8"
   },
   "outputs": [],
   "source": [
    "for i in range(y_test.shape[0]):  \n",
    "  #print([steering_angle[i][0], tf_y_angle[i][0]])\n",
    "  print([speed[i][0], y_test[i] ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUAC8BLCx-P3"
   },
   "outputs": [],
   "source": [
    "x= 0\n",
    "for i in range(y_test.shape[0]):\n",
    "  x += speed[i][0]-y_test[i]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8DWQcuLXWZa"
   },
   "outputs": [],
   "source": [
    "plt.hist(y_speed_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM8rZ_tsVErl"
   },
   "source": [
    "Calculate the Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuMrW57kVDtt"
   },
   "outputs": [],
   "source": [
    "mseSum = 0\n",
    "print(y_test.shape[0])\n",
    "for i in range(y_test.shape[0]):\n",
    "  mseSum += (1/y_test.shape[0] ) *  (steering_angle[i][0] - y_test[i])**2\n",
    "print(mseSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prH-bsTHprU2"
   },
   "outputs": [],
   "source": [
    "model.save(os.path.join(path,'Speed_prediction_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idYlPkdaSN24"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6prK2hyYsXJC"
   },
   "outputs": [],
   "source": [
    "model = load_model(path+'/Angle_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sz68hBEbwaSs"
   },
   "source": [
    "Now save the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhLfjNZawcqG"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "# always save model output as soon as model finishes training\n",
    "#model.save(os.path.join(path,'angle_prediction.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98lT73S6l6FH"
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=X.shape[1:]))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "# model.add(Dense(32))\n",
    "\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X, y_speed, batch_size=32, epochs=2, validation_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce6_nPCcS_Qm"
   },
   "source": [
    "Run the same model as above for angle prediction but change the loss function since we do not have classification. ALso accuracy is not a term to measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCeEN-rkkr-Y"
   },
   "outputs": [],
   "source": [
    "#y_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpfoFSn5Td66"
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=X.shape[1:]))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "# model.add(Dense(64))\n",
    "\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='adam')\n",
    "\n",
    "# model.fit(X, y_angle, batch_size=32, epochs=2, validation_split=0.95)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TrainSpeed.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
